   
# Backpropagation: An algorithm to optimize and evaluate the gradient of a loss function with respect to the weights. This allows iteratively tuning the weights of a Neural Network to minimize the loss function, thereby improving its accuracy.

- A deep dive into backpropagation to build a small autograd to gain a better understanding of multivariable derivatives, the chain rule, and minimizing the loss.
- Reference: Youtube by [AndrewjKarpathy](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)

# Why
- After implementing  the following repos:
  - Neural Network from scratch using only Numpy: https://github.com/SrEntropy/numpy-nn-from-scratch
  - Neural Network from scratch using pyTorch Tensors: https://github.com/SrEntropy/pytorch-tensors-from-scratch
- I realize that to fine-tune the loss function, I need to understand more about the backpropagation algorithm.
